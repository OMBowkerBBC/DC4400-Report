\section{Outputs}
  In this penultimate section, I will share some of the outputs the project has delivered. I will use these to reflect on the different parts of the project
  each output is associated with. To do this reflection I will use the Experience, Reflect, Action (ESA) framework founded by Melanie Jasper (2003, p.2).

  \begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/eraReflection.png}
    \caption{ERA reflection model founded by Jasper (2003, p.2).}
    \label{fig:eraReflection}
  \end{figure}  

  \subsection{Burn-up charts}
  \label{sec:burnup}
  
  \footnote{\textbf{SE-S05} - Burn up charts created helped to keep this project in specific time constraints and when the project would be completed.}
  \footnote{\textbf{SE-K06} - This shows how metrics can be gathered around the development lifecycle to track project progress and estimate finish dates.}
  \textbf{Experience} - Burn-up charts were generated throughout the project to help manage scope creep and issues, indicate progress made and help
  forecast a finish date (Atlassian, 2024a). This latter focus reason is especially important as this will allows us to give both partners and other 
  teams estimates on when new features/services plan to be released. They consist of three graphs, one to track total work, one to track work complete 
  and another is used to show where the project should be throughout the weeks (Fehlmann and Kranich, 2020). These graphs are calculated through Jira 
  and story points. Tasks are assigned a story point value dependant on how long that task is expected to take. These graphs can then be created when 
  these tickets are both closed/created and average velocity of the team can be calculated for future predictions.

  \vspace{0.2cm}
  \textbf{Reflection} - This is not the first time we've used burn-up charts, but we had more focus on them for this project. In the past we've been 
  lazy about sizing tickets so that they appear in the burn-up charts which has led to projects taking longer than anticipated due to these not 
  being in the forecast.

  This time round we did a better job at this in some respects but there are improvements to be made. Looking at the burn-up charts towards the end of the 
  project there was a huge decrease in work left to do. This was because a lot of work that was ticketed was deemed unnecessary. This is great in terms of 
  project finish time, however, undermines the process and points to issues in how we identify work at the beginning of a project.

  Another anomaly is that work completed plateaus in the middle of December. This is due to BBC policy around change freezes (OpenCRM, 2024) happening 
  during this time to not risk any services over the Christmas period. After this time work picks back up and reaches the estimated levels once again.

  \vspace{0.2cm}
  \textbf{Action} - I think the burn-up charts are great and being able to alert other teams and partners to expected completion dates is extremely
  valuable for integrations with our systems. However, we need to take more care when identifying tasks to do. In this project this phase was done 
  extremely quickly to get the project started. This led to duplicate and unnecessary tasks hampering our predictions. This also works the other way, 
  there were times during this project large tasks came up that we hadn't identified. For future it would save time to take time in this area and make these 
  burn-up charts more valuable and accurate. This extra time would also surface tasks that would \textit{'asteroid'} in along the way which cause disruption 
  and halts in development.

  \begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{assets/outputs/burnups/finalBurnUp.png}
    \caption{Final burn-up chart showing the MVP as complete with some post MVP tasks left to do, extra burn-up charts can be seen in 
    \hyperref[sec:AppendixG]{\textbf{Appendix G}}}.
    \label{fig:finalBurnUp}
  \end{figure}

  \newpage
  \subsection{Final System Architecture}

  \textbf{Experience} - 
  \footnote{\textbf{SE-S03} - This is one of the final outputs that show how the system was delivered using a wide range of technologies.}
  A diagram of the final architecture has been created to help both current and new team members understand how the system works.
  It's important to note that the old static lambda and associated schedule events and keyspaces will be removed over time as confidence in the new system 
  grows. This diagram will be used in our internal documentation, such as technical designs and threat models.

  \vspace{0.2cm}
  \textbf{Reflection} - For this project we had an initial design that was created during the spike, which consisted of a publish/subscribe model using
  AWS SNS and SQS (Amazon Web Services, 2024b and 2023c). Other than a separate queue to handle internal re-queuing of events and a garbage collector to 
  extract and simplify some logic, the main architecture didn't change. I think this was a mistake. There were two keys issues the spike found with the design,
  these being complexity of copying over data from the catalogue Redis to the schedule Redis and an inability to parallelise notifications. A solution to this 
  was offered and in the \hyperref[sec:dynamo]{\textbf{next section}} of this report I will discuss it. 

  The reasons for not going down this alternate approach was in part due to the spike taking longer than expected. Due to this there was no priority placed 
  on further discovery of this alternative solution, as we wanted to start the work straight away. The copying over of data hurt development time due to 
  its complexity and caused countless bugs and errors that could have been avoided.

  Despite all this it's extremely important to note that the system works exactly as intended and meets all the criteria that was expected. Schedules are 
  now much more up to date and thanks to comparison tests we can see that the output is correct.

  \vspace{0.2cm}
  \textbf{Action} - Time was not on our side for this project. There was a lot of pressure to start this project as soon as possible to finish off the 
  schedule pipeline work and have it ready for partners to use.

  Scope creep (Martins, 2023) and lack of time-boxing for the spike ate away at this time and meant later findings in the spike were not explored and we 
  instead opted for the architecture that was similar to the catalogue pipeline. Future spikes should be time-boxed to stop this creep, 
  so we can review findings earlier and explore a different direction if one has been identified.

  The alternate design also used new AWS services we hadn't used before. More time was needed to research this and understand it. Better spike planning 
  would help, but more general training around AWS services and how to implement them could be encouraged through official training or through side-project
  time (Robinson, 2018), which is something the BBC does offer.

  \begin{landscape}
    \begin{figure}[H]
      \centering
      \includegraphics[width=20cm]{assets/outputs/finalArchitecture.png}
      \caption{Final architecture for project.}
      \label{fig:finalArchitecture}
    \end{figure}
  \end{landscape}

  \newpage
  \subsection{Dashboard}

  \footnote{\textbf{SE-K06} - This shows how metrics can be gathered and used to monitor live software.}
  \textbf{Experience} - Dashboards have been created using a tool called Grafana (2024) that pulls metrics (Amazon Web Services, 2024k) from AWS and
  displays them all in one place. This allows us to check for anomalies and errors. Some metrics such as lambda run-times and invocations are provided by 
  AWS, but custom metrics have also been created to track errors, Redis writes and schedule re-queues. Metrics are collected during the run time of the 
  lambda using variations of the following code.

   \begin{lstlisting}[caption=Code used to update a metric\, this variation tracks a schedule delete.]
    monitoring.collect({
      typeId: 'Processing',
      result: 'throughput',
      dimensions: {
        ObjType: 'service_schedule',
        EventType: 'Delete'
      },
      value: 1
    })
  \end{lstlisting}

  \textbf{Reflection} - I've used metrics and dashboards throughout my time at the BBC, so this is not something that is new to me. Usually, the process of 
  adding these metrics is done at the very end of development. This stops monitoring during development which is only a negative. Due to the use of a spike 
  in this project we were able to know what metrics we wanted to track from the start and therefore added them along the way. This helped a lot as metrics on 
  both test and live could be compared to one another to validate that deployed changes had taken effect.
  
  \vspace{0.2cm}
  \textbf{Action} - Continue to use spikes where necessary and put more thought into what kind of metrics we want to track before development starts.
  This allows comparisons between different dashboards as well as saves one or multiple extra tasks at the end of development to create the metrics and
  dashboards.

  \begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{assets/outputs/dashboard.png}
    \caption{Created dashboard in Grafana.}
    \label{fig:dashboard}
  \end{figure}
  
  \newpage
  \subsection{Code Base and Commits}

  \textbf{Experience} - The project was created using JavaScript, alongside CloudFormation (Amazon Web Services, 2024l) for AWS resource provisioning and 
  Jenkins (2024) pipelines for CI/CD. This is our typical stack for all projects in our team. All unit tests were written using the Jest 
  (Meta Platforms, Inc, 2024) framework and were done alongside the development of tasks.

  \vspace{0.2cm}
  \textbf{Reflection} - Despite there being some questions over the architectural design, the team worked well to develop the final product, due to daily 
  discussion on how to tackle the problems that came up. It also helped that we stuck to known technologies and followed a similar working pattern to what 
  we usually do. In this aspect it could be argued that the right decision was made on architecture, as we could re-use a lot of code from elsewhere.
  However, as previously mentioned though it would be good to get more expertise and knowledge of newer/different solutions.

  \footnote{\textbf{SE-K05} - The following suggests improvements as well as things we already do as a team to improve code quality in the future.}
  There have been some talks through retrospectives (Atlassian, 2024b) about using TypeScript (Microsoft, 2024) for our future projects. TypeScript has been 
  shown to lead to better code quality, less bugs and more understandable code (Merkel, 2021). In addition to this, statically typed languages have been shown
  to help with bug fixes, especially around types (Okon and Hanenberg, 2016 and Kleinschmager et al, 2012).

  Once again, the code works as expected and provides the correct output. New technologies and languages should be regarded as ways to improve in the future.
  Overall, I'm happy with the work we did as a team, we ended up beating the estimated finish time by some way that was predicted by the burn-up charts and 
  I'm also happy that despite having to do a lot of other non-dev related tasks, I still managed to get a lot of work committed to the project.

  As a team we also tried to adhere to the Do not Repeat Yourself (DRY) principle throughout the project (Morais, 2023). This helps reduce number of lines of 
  code and stops the need to change code in multiple places due to slight changes in implementation detail. This could be seen in our decisions around the 
  coldstart in delta mode, as well as the shared API used to updated schedule items on catalogue update notifications.

  \vspace{0.2cm}
  \textbf{Action} - TypeScript would help with code quality and other aspects in the future. However, it is not worth re-writing everything at this
  stage and instead consider for new projects in the future. As previously mentioned, continuous learning of new technologies is required to create the 
  best products possible, both from and external and internal perspective. 
  
  Continued retrospectives will help bring up some of these technologies and any other issue the team has.
  \footnote{\textbf{SE-S06} - Image below shows that I have been involved in code reviews.}

  \begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{assets/outputs/githubContributions.png}
    \includegraphics[width=6cm]{assets/outputs/scheduleGeneratorChanges.png}

    \caption{GitHub contributions from my work profile and the schedule generator project.}
    \label{fig:githubStats}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/outputs/jiraTasksCompleted.png}
    \caption{Jira epic showing all tasks completed.}
    \label{fig:jiraTasksCompleted}
  \end{figure}
  
\newpage
